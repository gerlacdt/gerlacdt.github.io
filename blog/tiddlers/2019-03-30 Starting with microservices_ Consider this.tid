created: 20190329182025676
modified: 20190331095422333
tags: blog programming
title: 2019-03-30 Starting with microservices? Consider this
type: text/vnd.tiddlywiki

!! Introduction

Some years ago i was involved in migrating a big IBM Websphere
monolith into a microservice landscape. We had a lot of problems with
the monolith. Our development speed slowed down immensely. We had many
merge conflicts because of too many dependencies in the
codebase. Basically we outgrew the monolithic design and we started to extract different domains like payment, ordering and search. The teams were restructured into Two-pizza teams.

The whole migration was very structured and well planned. We migrated
incrementally with the [[strangler pattern|https://docs.microsoft.com/en-us/azure/architecture/patterns/strangler]].
And as long we were ~20 developers, everything was fine. Teams worked
independently in their domain. They could scale, deploy independently
and followed their own release cycles.

But microservices bring new technical challenges. Maybe you now the saying:

<<<
Microservices solve organizational problems.

Microservices cause technical problems.
<<<[[Peter Bourgon|https://speakerdeck.com/peterbourgon/go-plus-microservices-equals-go-kit?slide=15]]

Developers tend to ignore, forget or just simply do not know about
these challenges. They are overwhelmed by the new opportunities
in the shiny microservice world. In the following i want to present my
experience what we underestimated or forgot in our
microservice-migration.


!! Avoid a heterogeneous IT-landscape

Microservices give you a lot of freedom which technology to choose
like programming language, database etc. This is both boon and bane. On
the one side every team can choose the technology which fits best, on
the other side developers are adventurous and try out new fancy
programming languages and other stuff. And soon you have a dozen
different tech stacks to support. Some teams build their own
tech-knowledge silos. And don't get me wrong here, it is totally fine
if a team has its domain-knowledge silo, but it is not fine if the team is
the only one in the company who uses a fancy programming language.  At google, they have thousands of engineers and they try to
stick to only 4 languages in the backend. The supported languages are
C++, Java, Python and Go, see
[[Software Engineering at Google, Fergus Henderson|https://arxiv.org/pdf/1702.01715.pdf]].
If a backend team wants to start with another language they need to argument hard why
they cannot fulfil their job with the company-wide supported
languages. There is a clear decision-process in place how to
introduce new technology. That makes it a lot harder for developers to get to experimental and restrict them to use the newest and bleeding-edge technology.
Another advantage from sticking to a minimum number of programming languages is that one can focus on business problems. For internal libraries you don't have to support a vast number of languages. Developers will also have a easy time when the want to rotate teams.

A [[technology radar|https://www.thoughtworks.com/de/radar]] serves as an overview for the supported supported tools, programming languages and platforms. You also need a clear decision process of how new tech-stacks get approved.


!! Have a good Monitoring/Tracing/Logging in place

We experience often logging problems because our ~ElasticSearch cluster is
overloaded or some indices are conflicting. Logging is hard. Make sure you have clear
logging-guildlines. Do not log too much, at best you have
[[silent services|https://peter.bourgon.org/blog/2016/02/07/logging-v-instrumentation.html]],
i.e. only log if a manual intervention is needed in case of an
error. Logging is fine for debugging and error tracking in the
Development-Stage but it should not be overused in Production. Be
careful if your monitoring and alarming is based on logging. If your
log-cluster is flooded, your monitoring will not work either!

Have strict rules for monitoring. No service should go into production
without monitoring. With microservices you need transparency in order
to do failure analysis or get notified if something is wrong. At least monitor
the [[four golden rules|https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/#xref_monitoring_golden-signals]]:

1. Latency. How high is the response time? Make sure you distinguish
   failed requests from successful requests. Failed requests can be
   fast and screw up your metrics.

2. Traffic. How many Requests per second (RPS) do we serve?

3.  Errors. How high is the error rate? Number of error response /
     Number of successful responses

4. Saturation. How full is your service? CPU utilisation, memory
    consumption.


!! Don' forget the 8 fallacies of distributed computing

[[Fallacies of distributed computing by Peter Deutsch|https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing]]

1. The network is reliable.

   Your HTTP-calls will fail from time to time. So make sure you have
   some retry-mechanism in place. But do not retry naively!
   E.g. retries make not sense if the http-response-code is 400
   BAD_REQUEST, no matter how often you try the request will never be successful. 
   Neither you should do retries on POST requests, HTTP POST requests
   are by definition not idempotent and you could accidentally create a
   lot of resources like orders and bookings. Your customers will not
   be happy.

   You should also limit your retries, do not retry indefinitely,
   otherwise you can cause overload on other services, see
   [[cascading failures|https://landing.google.com/sre/sre-book/chapters/addressing-cascading-failures/]]

   Do not forget to secure your services with timeouts. Missing timeouts
   make I/O calls wait too long and if they pile up they consume all
   your memory. In the worst case you service will be killed by your
   docker scheduler because the memory limit is exceeded.

   For very high-load services you should consider 
[[load shedding or graceful degradation|https://landing.google.com/sre/sre-book/chapters/addressing-cascading-failures/#xref_cascading-failure_load-shed-graceful-degredation]] in order to protect these service from going down.

2. Latency is zero.

   Network can be slow. You should clearly separate internal function
   calls from network calls. E.g. do not use network calls in
   loops. Better fetch everything with one network call.

```javascript
// bad practice! Doing multiple network calls
function getUsers(userIds) {
  const users = [];
  for (u of users) {
    const user = http.fetchSingleUser(u);
    users.push(user);
  }
  return users;
}

// good, only one network call
function getUsers(userIds) {
  return http.fetchUsers(userIds);
}
```

3. Bandwidth is infinite.

   Use your bandwidth sparingly. E.g logging every http request and
   response is definitely too much. Do not use  sql-statements like this: 
`SELECT * from  table`, always select only required columns.

4. The network is secure.

5. Topology doesn't change.

   Especially with self-healing services and docker random port usage,
   IP-addresses and ports change. Do not use static IP-addresses or
   static ports. Use sophisticated service-discovery mechanisms. Many
   Docker-Scheduler provide them out-of-the-box like
   [[Kubernetes|https://kubernetes.io/]] or 
[[Hashicorp's Nomad|https://www.nomadproject.io/]]. With
   [[AWS ECS|https://aws.amazon.com/ecs/]] you can use an
   [[AWS Application Load Balancer|https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html]]
   (ALB) for service discovery.

6. There is one administrator.

   Distributed systems are complex. So do not expected that there is
   one person who knows it all. For bug analysis you need multiple
   people/teams.

7. Transport cost is zero.

   Network calls are expensive. You need to establish a connection,
   you need to serialize and deserialize the message body which cost
   CPU. So the less you send the better. If you notice that you
   services are chatty, consider restructuring the domain
   boundaries. Most probably your domains do overlap or have multiple
   responsibilities.


8. The network is homogeneous

   It's not. Before docker, almost all application ran on customized, "snowlflaky" servers 
   configured via non-reproducable ssh-session. With Kubernetes or AWS ECS you can have 
   clusters with thousands of servers which are configured
   exactly the same. A single ~DevOps guys can operate thousands of
   servers! What an achievement! Make sure you use these tools. Also
   make sure you agree on a single data transfer format like
   [[JSON|https://www.json.org/]] or [[Protocol Buffers|https://developers.google.com/protocol-buffers/]] with
   [[gRPC|https://grpc.io/]].


!! Other things to consider

- Keep your domain boundaries clear and separated. Teams should always
  be able to deploy independently and without consulting other
  teams. You ever experienced a deployment where 3 or 4 services were
  involved and had to be deployed synchronously because of
  incompatibilities of API-versions or dependency on the same database. 
  Congratulations you built a distributed monolith.

- Have a good CI/CD pipeline with automatic tests and deployments. 
  Make sure all teams use the same
  deployment and glue scripts. Use one Docker-Cluster
  (Kubernetes, AWS ECS, Hashicorp Nomad) for the whole company (as
  long as you do not exceed hundreds of servers). This will keep the
  maintaining effort low.

- If you have reusable modules/libraries, make sure you have
  transitive CI-builds

- Adhere to good coding cloud-native practices, see
  [[12-factor-app|https://12factor.net]] and
  [[Site Reliability Engineering|https://landing.google.com/sre/books/]]


You made it this far! Thanks for reading.